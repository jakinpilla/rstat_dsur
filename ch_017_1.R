#' ---
#' title: "ch017_1(탐색적인자분석)"
#' author: "jakinpilla"
#' date : "`r format(Sys.time(), '%Y-%m-%d')`"
#' output: 
#'    github_document : 
#'        pandoc_args: --webtex
#'        toc : true
#'        toc_depth : 5
#' ---


#+ message = FALSE, warning = FALSE
library(ez)
library(ggplot2)
library(nlme)
library(pastecs)
library(reshape2)
library(WRS)
library(clinfun)
library(pgirmess)
library(car)
library(tidyverse)
# install.packages('mvoutlier')
library(mvoutlier)


#' #### 이번 장에서 배우는 내용
#' 
#' 인자분석
#' 
#' #### 인자분석은 언제 사용하는가?
#' 
#' latent variable: 잠재변수 /직접 측정할 수 없는 것
#' 
#' 인자분석을 통해 (1) 일단의 변수들의 구조를 이해하는 것, (2) 바탕 변수를 측정하는 설문을 구축하는 것, (3) 주어진 자료 집합을, 원래의 정보를 최재한 유지하면서도 좀 더 다루기 쉬운 크기로 줄이는 것 등을 할 수 있다.
#' 
#' #### 인자
#' 
#' 만일 변수들의 부분집합들 사이의 서로 연관된 변수들의 군집에서 비롯된 자료 집합의 군집을 형성하고 있다면, 그런 변수들이 동일한 바탕 차원의 여러 측면을 측정한 것일 가능성이 있다. 이러한 바탕 차원을 인자(factor)라고 부른다. (잠재변수라고도 한다.)
#' 
#' 인자분석(factor analysis)은 그보다 작은 크기의 인자 집합으로 축약된다. 이를 통해서 상관 행렬에 있는 공통의 변동을 최소한의 탐색적 구인(explanatory construct)들을 이용해서 최대한 많이 설명한다는 극도의 절약이 이루어진다.
#' 
#' 심리학의 특성론(trait theory) 연구자들은 성격의 특성을 평가할 때 인자분석을 즐겨 사용한다.
#' 
#' #### 인자의 시각적 표현
#' 
#' 그래프의 분류축으로 시각화, 한 변수의 위치는 두 인자와의 상관 정도에 따라 결정된다.
#' 
#' 같은 축에 대해 좌표성분이 큰 변수들은 어떤 공통의 바탕 측명의 서로 다른 측명을 측정한 것이라고 가정할 수 있다. 분류축에 따른 한 변수의 좌표성분을 인자적재값(factor loading)이라고 부른다.
#' 
#' 
#' #### 인자의 수학적 표현
#' 
#' 인자를 서술하는 직선방정식은 선형모형을 서술하는 직선방정식과 본질적으로 같다.
#' 
#' 인자행렬(인자분석), 성분행렬(주성분분석)
#' 
#' #### 인자점수
#' 
#' factor score
#' 
#' 
#' ##### 회귀법
#' 
#' b들에 인자적재값들이 아니라 인자점수 계수들을 대입한다. 그러한 여러 인자점수 계산법은 인자점수 계수들을 대입한다는 점이 다르다. 인자 점수 계산법은 인자점수 계수를 계산하는 방법에 따라 나뉘는데, 가장 간단한 방법은 회귀법이다. 이 방법에서는 변수들의 초기 상관관계를 고려해서 인자적재값들을 수정한다. 이에 의해 측정 단위의 차이가 사라지고 변수 분산들이 안정화된다.
#' 
#' 인자점수 계수들의 행렬(B)
#' 
#' 상관행렬(R)
#' 
#' 인자적재값 행렬(A)
#' 
#' $$B = R^{-1} A$$
#' 
#' 
#' 인자적재값 행렬에 상관행렬의 역행렬을 곱한다는 것은 개념적으로 각 인자적재값을 그에 대응되는 각 상관계수로 나누는 것과 같다. 결과적인 인자점수행렬은 원래의 변수 쌍들의 관계를 고려해서 수정된 각 변수와 각 인자의 관계를 나타낸다. 그런 차원에서 이 행렬은 변수들과 인자들의 유일한 관계를 좀 더 순수하게 대표하는 측도라 할 수 있다.
#' 
#' #### 인자점수의 용도
#' 
#' - 1) 원래의 점수들 대신 인자점수들에 대해 추가적인 분석을 수행할 수 있다.
#' - 2) 회귀분석의 공선성 문제를 극복할 수 있다.
#' 
#' #### 인자를 찾아내는 여러 방법 
#' 
#' 목적에 따라 인자를 찾아내는 방법이 달라짐
#'  - 첫째, 자료 표본에서 발견한 사실들을 모집단으로 일반화할 것이지...
#'  - 둘째, 특정 가설없이 자료를 탐색하는 것이 목적인지 아니면 어떤 특정한 가설을 검증하려는 것인지...
#'  
#'  특정 기법들은 사용된 표본이 곧 모집합이며, 분석 결과를 해당 표본 이상으로는 일반화할 수는 없다고 가정한다. (주성분분석, 주축 인자분석 등)
#' 
#' 한편, 참가자들이 무작위로 선택되었으며 측정된 변수들이 우리가 관심이 있는 변수들의 모집단을 구성한다고 가정하는 접근 방식들도 있다. 그러한 가정 하에서는 표본에서 얻은 결과를 그 보다 큰 모집단으로 일반화하는 기법이 가능하다. 그러나 모든 발견은 오직 측정된 변수들의 집합에 대해서만 참이라는 제약이 따른다.(최대가능도법, 알파 인자추출 등)
#' 
#' #### 공통성
#' 
#' 특정 변수의 총 변동은 두 부분으로 구성되는데, 하나는 다른 변수(측도)들과 공유하는 부분이고, 다른 하나는 그 변수에만 해당하는 부분이다. 전자를 공통분산(common variance)이라고 부르고 후자를 유일분산(unique varianve)라고 부른다. 또한 신뢰성 있게 설명할 수 없는 분산도 있는데 이러한 분산을 임의분산(random variance)라고 부른다.
#' 
#' 인자분석에서 우리의 관심사는 자료에 있는 공통의 바탕 차원들을 발견하는 것이므로, 기본적으로는 공통분산에만 관심을 둔다.
#' 
#' 첫째는 분산 전체가 공통분산이라고 가정하는 것이다. 모든 변수의 공통성이 1이라고 가정한다. 이러한 가정에서 인자분석을 수행하는 것은 원래의 자료를 여러 선형 성분들로 분해하는 것에 해당한다. 이를 주성분분석이라고 한다.
#' 
#' 둘째는 각 변수의 공통성 값을 추정함으로써 공통분산의 양을 추정하는 것이다. 공통성을 추정하는 방법은 여러가지인데, 각 변수와 다른 모든 변수의 다중상관곱을 이용하는 방법들이 있다.(알파 인자추출 등)
#' 
#' #### 주성분분석의 이론
#' 
#' 주성분분석은 다변량분산분석과 비슷한 방식으로 진행된다. 우선 변수들의 관계를 나타내는 행렬을 구하고, 그 행렬의 고유값들을 계산해서 행렬의 선형 성분(변량 또는 인자)들을 구한다. 그로부터 고유벡터들을 구하는데 고유벡터의 성분들은 해당 인자의 각 변수의 인자적재값이다. 또한 고유값은 해당 고유벡터의  실질적 중요도를 나타낸다.
#' 
#' #### 인자추출: 고유값과 scree plot
#' 
#' 
#' #### 해석의 개선: 인자회전
#' 
#' 사각회전은 바탕 인자들이 이론적으로 관련이 있을 만한 좋은 이유가 있을 때만 사용해야 한다.
#' 
#' 인자변환행렬(factor transformation matrix)이 필요하다.
#' 
#' #### 인자 회전 방법의 선택
#' 
#' R은 직교회전을 위한 방법 네 가지와 사각회전을 위한 다섯가지 방법을 지원한다.
#' 
#' - 직교회전: varimax, quartimax, BartlerT, geominT
#' - 사각회전: oblimin, promax, simplimax, BentlerQ, geominQ
#' - 쿼티맥스(quartimax): 한 변수의 인자적재값들이 모든 인자에 대해 최대한 넓게 퍼지는 회전각을 찾음
#' - 배리맥스(varimax): 인자들 안에서의 적재값들의 분산이 최대가 되는 각도를 찾음.
#' - 프로맥스(promax):프로맥스는 계산이 빠르다. 이 방법은 아주 큰 자료 집합에 맞게 설계된 것임
#' - 인자들이 상관되어 있다고 가정할 이론적 근거가 있다면 직접 오블리민(direct oblimin)을 선택한다.
#' 
#' #### 인자적재값의 실질적 중요도
#' 
#' 인자적재값은 주어진 인자에 대한 주어진 변수의 실질적 중요도를 나타내는 측도이다.
#' 
#' 스티븐스는 표본 크기가 50이면 .722보다 큰 인자적재값을 유의하다고 간주하고, 표본크기가 100이면 .512, 표본크기가 200이면 .364, 600이면 .21, 1000이면 .162보다 큰 인자적재값을 유의하다고 간주하였다.
#' 
#' 해당변수의 중요도는 인자적재값을 제곱해서 구할 수 있는데, 그러한 제곱은 인자의 변동 중 변수가 설명하는 부분의 비율에 대한 추정값에 해당한다.(결정계수와 비슷하다.)
#' 
#' #### 연구예제
#' 
#' 인자분석의 한 가지 용도는 설문지 개발이다.
#' 
#' 피험자의 어떤 능력이나 특성을 측정하려면 피험자에게 묻는 질문들이 측정하고자 하는 구인(construct)과 관련이 있어야 한다.
#' 
#' #### 표본크기
#' 
#' 변수당 참가자 수가 적어도 10~15는 되어야 한다.
#' 
#' 신뢰성 있는 인자 해를 결정하는데 가장 중요한 것은 절대 표본 크기와 인자적재값의 절대크기이다.
#' 
#' 공통성이 작으면 표본 크기의 중요성이 증가한다.
#' 
#' 표본 크기가 300이상이면 안정적인 해가 나올 가능성이 크다.
#' 
#' KMO: 변수들 사이의 상관계수의 제곱과 변수들 사이의 편상관계수의 제곱의 비를 나타냄.
#' 
#' KMO가 0이면 편상관계수들의 합이 상관계수들의 합에 비해 큰 것인데, 이는 상관계수들이 넓게 퍼져 있는 패턴에 해당한다.
#' 
#' KMO가 1에 가까우면 상관계수들이 비교적 한데 몰려 있는 것이며, 신뢰성 있는 인자들이 나올 가능성이 크다.
#' 
#' #### 변수들의 상관관계
#' 
#' 상관관계가 충분히 높지 않은 것과 상관관계가 너무 높은 것 모두 문제이다.
#' 
#' ##### 상관관계가 충분히 높지 않은 것
#' 
#' 바틀렛 검정: 모집단의 상관행렬이 단위행렬과 비슷한지를 검사한다. 단위행렬은 비대각성분이 모두 0인 행렬이다. 모집단 상관행렬이 단위행렬을 닮았다는 것은 모든 변수가 다른 모든 변수와 별로 관계가 없다는 뜻이다. 바틀럿 검정은 자료의 상관행렬이 단위행렬과 유의하게 다른지를 검사한다. 따라서, 바틀렛 검정 결과가 유의하다면 변수들의 상관계수들이 0와 유의하게 다른 것이다. 즉 바틀렛 검정결과가 유의하면 좋은 것이다.
#' 
#' ##### 상관관계가 너무 높은 것
#' 
#' 적당한 수준의 다중공선성은 인자분석에 문제가 되지 않지만, 다중공선성이 너무 높은 상황과 특이성 즉 변수들이 완벽하게 상관된 상황은 피해야 한다.
#' 
#' 다중공선성이 존재하면, 상관관계가 높은 변수들로 이루어진 인자에 대한 유일 기여를 결정하는 것이 불가능해진다.
#' 
#' 다중공선성은 $R$ 행렬의 판별식으로 검출할 수 있다. 
#' 
#' 상관행렬에 다중공선성 분제가 존재한다는 근거를 확보했다면, 분석을 더 진행하기 전에 먼저 상관행렬에서 상관계수가 아주 큰 변수들을 찾아서 둘 중 하나를 제거하는 것을 고려할 필요가 있다.
#' 
#' #### 자료의 분포
#' 
#' 인자분석을 위해서는 변수들의 상관관계가 적당해야 할 뿐만 아니라, 변수들이 대체로 정규분포를 따라야 한다.
#' 
#' 분석결과를 수집된 표본 이상으로 일반화하려 할 때에는 정규성 가정이 가장 중요하다.
#' 
#' 이분적 변수들이 있다면 다분상관계수들로 상관행렬을 만들어야 한다.
#' 
#' 
#' #### R Commander를 이용한 인자분석 실행
#' 
#' 비추천
#' 
#' #### R을 이용한 인자분석 실행
#' 
# install.packages('corpcor')
# install.packages('GPArotation')

library(corpcor)
library(GPArotation)
library(psych)


#' #### 초기 분석 및 분석
#' 
raqData <- read.delim('raq.dat', header = T)

raqMatrix <- cor(raqData)

round(raqMatrix, 2)

cortest.bartlett(raqData)

#' ##### 바틀렛 검정
#' 
#' $\chi^{2}(253) = 19.334, p < .001$이므로 통계적으로 유의하다. 즉 이 R 행렬이 단위행렬과 다르다는 대립가설을 채택할 수 있다. 따라서 이 자료는 인자분석을 하기에 적합하다.
#' 
#' ##### KMO 검정
#' 
library(DSUR.noof)
kmo(raqData)

#' 전반적인 KMO 값은 .93인데, 이는 뛰어난 범위에 해당한다. 따라서 지금 예제의 표본 크기와 자료가 인자분석에 적합하다고 확신해도 좋다.
#' 
#' ##### 상관행렬의 판별식
#' 
det(raqMatrix)
raqData %>% cor()  %>% det() 

#' - 상관행렬에서 다른 어떤 변수와도 상관되지 않은 변수나 하나 이상의 변수와 아주 높게 상관된 변수를 찾아본다. 그리고 인자분석의 경우에는 상관행렬의 판별식이 .00001보다 큰 지 확인한다. .00001보다 크다면 다중공선성 문제는 없는 것이다.
#' 
#' - KMO 검정과 바클렛 검정을 수행해서, 해당 검정통계량들이 최소한의 값인 .5보다 큰 지 확인한다. 만일 .5보다 크지 않다면 자료를 더 수집해 봐야 할 것이다. 이후 과정을 위해서는 바틀렛의 구형성 검정 결과가 반드시 유의해야 한다.
#' 
pc1 <- principal(raqData, nfactors=23, rotate = 'none')
pc1 <- principal(raqMatrix, nfactors = 23, rotate = 'none')

length(raqData)

#' h2는 공통성, u2는 유일성을 의미
#' 
#' u2: 이 값에는 각 변수에 대해 고유한 변동의 양을 나타낸다. 그 값은 1 빼기 공통성 값과 값다.
#' 
#' 출력 결과에서 SS loadings 행에 나온 수치들이 바로 각 인자의 고유값이다.
#' 
#' Cumulative Propotion에서 인자 1은 총 변동의 32%를 설명한다. 
#' 
#' 고유값이 1보다 큰 성분이 네 개 있다. 카이저 기준을 적용한다면 네 개의 인자를 추출해야 한다.
#' 
plot(pc1$values, type = 'b')

#' 4개의 성분으로 정할 수 있다.
#' 
pc2 <- principal(raqData, nfactors = 4, rotate = 'none')


pc2
pc2 %>% str()



#' h2, u2 값이 달라졌음을 알 수 있다. 
#' 
#' 공통성은 주어진 변수 안에 있는 공통분산의 비율이다. 초기 주성분분석은 모든 분산이 같다고 가정한다. 즉, 추출 이전에는 모든 공통성 값이 1이다. 본질적으로 이는 한 변수에 연관된 모든 분산이 공통분산이라고 가정하는 것에 해당한다. 그러나 특정 인자들을 추출하고 나면, 공통분산이 실제로 어느 정도나 되는지를 좀 더 정확하게 알 수 있다. 출력의 공통성 값은 바로 그러한 공통분산을 반영한 수치이다.
#' 
#' 공통성 값을 바탕 인자들이 설명하는 변동의 비율이라고 이해할 수도 있다. 추출 이후의 공통성 값들은 각 변수의 변동 중 유지된 인자들이 설명할 수 있는 부분의 양을 나타낸다.
#' 
#' 카이저 기준은 변수가 30개 미만이고 추출 이후의 공통성들이 모두 .7보다 크다는 조건을 만족하거나 표본 크기가 250을 넘고 평균 공통성이 .6보다 크다는 조건을 만족하는 경우에 정확하다. 
#' 
#' 자료에서는 .7보다 큰 것은 하나뿐이고 출력 자료의 공통성 값들의 평균은 .503 이므로 카이저 기준이 정확하지 않을 수 있다.
#' 
#' 추출한 인자 갯수가 적절했는지 파악하는 또다른 방법은 자료의 상관행렬과 모형에 의해 재산출된 상관행렬을 비교하는 것이다.
#' 
factor.model(pc2$loadings) # 재산출된 상관행렬

#' 재산출 상관행렬의 대각성분들이 추출 이후의 각 변수의 공통성 값이다.

#' 재산출된 상관행렬과 실제 상관행렬의 차이를 잔차행렬이라고 부르는데 `factor.residuals()` 함수로 구할 수 있다.
#' 
factor.residuals(raqMatrix, pc2$loadings) # 잔차 행렬

#' 잔차행렬의 대각성분들이 각 변수의 유일성 값이다.
#' 
#' 재산출된 상관행렬의 상관계수들이 원래의 R 행렬의 상관계수들과 다른 이유는, 재산출된 상관계수들은 관측된 자료가 아니라 모형에서 비롯된 것이기 때문이다. 만일 모형이 자료에 완벽하게 적합한다면 재산출된 상관계수들이 원래의 상수들과 일치할 것이다. 따라서 관측된 상관계수들과 모형의 상관계수들의 차이는 곧 모형이 자료에 얼라마 잘 적합하는지를 나타내는 척도라고 할 수 있다.
#' 
#' 모형의 적합도를 평가하는 한 측도는 잔차 제곱들의 합을 상관계구 제곱들의 합으로 나눈 것이다.
#' 
#' 그런데 모형 적합도의 측도라고 한다면 0이 최악의 모형이고 1이 최선의 모형을 뜻하는 것이 자연스러우므로, 1에서 그 값을 뺀 결과를 측도라 사용하는 것이 바람직하다. 주성분분석 결과의 제일 끝에 나온 다음의 항목이 바로 이 측도이다.
#' 
#' $$Fit based upon off diagonal values = 0.96$$
#' 
#' 이 값이 .95보다 크면 모형의 적합도가 좋은 것으로 간주한다.
#' 
residuals <- factor.residuals(raqMatrix, pc2$loadings)

#' 이 행렬의 상삼각부분, 즉 주대각의 위쪽 성분들만 추출한다.
#' 
residuals <- as.matrix(residuals[upper.tri(residuals)])

large.resid <- abs(residuals) > .05
large.resid %>% sum()

sum(large.resid) / nrow(residuals)

#' 약 36% .05 보다 크다. 만일 .05보다 큰 잔차가 50%이상이면 자료를 걱정해보아야 한다. 
#' 
#' 잔차들을 평가하는 또 다른 방법은 그 평균을 보는 것이다. 잔차 제곱들의 평균을 구해서 제곱근을 취하는 것이 바람직하다. 이를 잔차의 제곱평균제곱근이라고 부른다.
#'
residuals^2 %>% mean() %>% sqrt()

#' 잔차들의 분포를 살펴보는 것도 좋다. 좋은 모형이라면 잔차들이 대체로 정규분포를 따를 것이다.

hist(residuals)
shapiro.test(residuals)

#' 이 예제에서 잔차들은 정규분포를 따른다.
#' 
#' #### 회전
#' 
#' 회전을 가하면 추출된 인자 중 하나에 대한 각 변수의 적재값이 최대화되고, 다른 모든 인자에 대한 적재값은 최소화된다. 회전은 변수들의 상대적 차이를 유지하면서 변수들의 절대값을 변경하는 식으로 일어난다.
#' 
#' 만일 인자들이 서로 독립이라고 믿을 만한 이론적 근거가 있다면 직교회전 방법 중 하나를 선택해야 한다. 그러나 이론적으로 인자들이 서로 관계가 있다고 예측하는 경우에는 사각회전 방법 중 하나를 선택해야 한다.
#' 
#' #### 요점정리
#' 
#' - 고유값과 산비탈그림을 보고 추출할 인자 개수를 결정한다.
#' 
#' - 변수가 30개 미만이고 공통성 값들이 모두 .7보다 크다면, 1보다 큰(카이저 기준) 고유값 개수를 추출할 인자 개수로 사용하면 된다. 또는 표본 크기가 250을 넘고 평균 공통성이 .6보타 클 때도 그 기준을 사용하면 된다. 아니면, 참가자들이 200 이상일 때는 산비탈그림을 보도 인자 개수를 정할 수도 있다.
#' 
#' - 잔차 행렬을 구하고, 절대값이 .05보다 큰 잔차가 50% 미만인지 확인해야 한다. 또한 모형 적합도가 .90보다 큰 지도 확인해야 한다.
#' 
#' #### 직교회전(배리맥스)
#' 
pc3 <- principal(raqData, nfactors = 4, rotate = 'varimax')
pc3 

#' 행렬의 성분들은 다르지만 h2 열(공통성 값)와 u2 열(유일성 값)은 변하지 않았음을 주목하자.
#' 
#' 인자들을 회전하면 인자들이 각 변수의 변동에 기여하는 정도가 달라진다. 하지만 변수들의 총변동 중 인자들이 설명하는 부분의 총량이 달라지지는 않는다.
#' 
#' 회전의 목적 중 하나는 고유값들을 좀 더 고르게 분포시키는 것이다. 그러나, 인자들을 회전해도 고유값들의 총합은 변하지 않는다.
#' 
print.psych(pc3, cut = .3, sort = T)

#' 이 명령은 pc3 모형의 인자적재값 행렬 중 크기가 .3 미만인 적재값들을 제거한 후 문항들을 내림차순으로 정렬한 결과를 출력한다.
#' 
#' 다음 단계는 같은 인자에 실린 질문들의 내용을 살펴보고 어떤 공통의 주제가 있는지 파악하는 것이다. 분석으로 산출된 수학적 인자가 실세계의 어떤 구인을 대표한다면, 같은 인자에 대해 인자적재값이 큰 질문들이 공유하는 어떤 공통의 주제를 고찰하는 것이 그 구인이 무엇인지 파악하는데 도움이 될 것이다.
#' 
#' - 인자1(RC3) : 컴퓨터 두려움
#' - 인자2(RC1) : 통계학 두려움
#' - 인자3(RC4) : 수학 두려움
#' - 인자4(RC2) : 동료 평가 두려움
#' 
#' #### 사각회전
#' 
#' 위에서 추출한 인자들은 두려움에 대한 인자들임으로 서로 상관되어 있을 가능성이 크다. 이처럼 인자들이 서로 연관되어 있다고 추측할 때는 직교회전이 아니라 사각회전을 적용해야 한다.
#' 
pc4 <- principal(raqData, nfactors = 4, rotate = 'oblimin')
pc4


#' 좀 더 보기 쉬운 형태로 변환하자.

print.psych(pc4, cut = .3, sort = T)

#' - 인자1(TC1) : 컴퓨터 두려움
#' - 인자2(TC4) : 동료 평가 두려움
#' - 인자3(TC3) : 통계학 두려움
#' - 인자4(TC2) : 수학 두려움
#' 
#' 이번에는 상관행렬이 있다. 직교회전에서는 R이 이 행렬을 출력하지 않았는데, 직교회전에서는 인자들의 상관계수들이 모두 0이기 때문이다.
#' 
#' 이 상관행렬들로 볼 때 이 자료의 인자들이 독립적이라고 말할 수는 없을 것이다. 따라서 직교회전의 결과는 폐기하는 것이 좋다. 그보다는 사각회전으로 얻은 해가 의미 있을 가능성이 더 크다. 
#' 
#' 사각회전을 가하면 인자행렬(성분행렬)은 두 행렬로 분리된다. 하나는 패턴행렬이고 하나는 구조행렬이다. 직교회전에서는 그 두 행렬이 같다. 패턴행렬은 인자적재값들로 구성되므로, 직교회전에서 해석한 인자적재값 행렬에 해당한다고 할 수 있다. 구조행렬은 인자들 사이의 관계를 고려해서 수정한 성분들을 담고 있다. (구체적으로 구조행렬은 패턴행렬과 상관행렬을 곱한 것이다.)
#' 
#' 인자적재값은 $pc4\\$loadings$에 들어있다. 그리고 인자들의 상관계수들은 $pc4$Phi$에 들어있다.
#' 
pc4$loadings %*% pc4$Phi

library(DSUR.noof)
# DSUR.noof::factor.structure(pc4, cut = .3) # 실행불가

pc5 <- principal(raqData, nfactors = 4, rotate = 'oblimin', scores = T)
pc5

#' $scores = T$ 옵션을 추가하면 인자점수들을 담은 scores라는 객체가 주성분 모형에 추가된다.
#' 
pc5$scores %>% head(10)

#' 이 인자점수들을 보면 다른 사람들의 두려움 정도에 비한 한 사람의 두려움 정도를 파악할 수 있다. 
#' 
#' 또한 인자점수들에 회귀분석을 적용할 수 있다. 만일 예측변수들의 상관관계가 유의하게 높다는 결과가 나왔다면, 인자들 사이에 다중공선성 문제가 있는 것이다.
#' 
raqData <- cbind(raqData, pc5$scores)

raqData %>% head()


#' #### 회전결과의 해석
#' 
#' - 직교회전을 적용했다면, 출력된 행렬에서 변수마다 어떤 인자(성분)에 대한 인자적재값이 가장 큰 지 살펴본다. 또한, 인자마다 그에 대한 인자 적재값이 큰 변수들도 살펴본다.(여기서 '큰'은 인자적잭밧의 절대값이 .4보다 큰 것을 말한다.) 같은 인자에 대해 인자적재값이 큰 변수들이 공통으로 나타내는 주제가 무엇인지 생각해본다.
#' 
#' - 사각회전을 적용했다면 출력된 행렬(패턴행렬)에서 변수마다 어떤 인자에 대한 인자적재값이 가장 큰지 살펴본다. 또한 인자마다 그에 대한 인자적재값이 큰 변수들도 살펴본다. 그로부터 발견한 것을 구조행렬을 계산해서 재점검한다. 같은 인자에 대해 인자적재값이 큰 변수들이 공통으로 나타내는 주제가 무엇인지 생각해본다.
#' 
